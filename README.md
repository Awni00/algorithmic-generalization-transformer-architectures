<div align="center">

# ğŸ§  Unlocking Out-of-Distribution Generalization in Transformers via Recursive Latent Space Reasoning

**Awni Altabaa** Â· **Siyu Chen** Â· **John Lafferty** Â· **Zhuoran Yang**

*Enhancing Transformer architectures with recursive latent space reasoning mechanisms for robust algorithmic generalization*

<!-- TODO: add arxiv link -->

</div>

---

## ğŸ“ Abstract

Systematic, compositional generalization beyond the training distribution remains a core challenge in machine learning---and a critical bottleneck for the emergent reasoning abilities of modern language models.

This work investigates out-of-distribution (OOD) generalization in Transformer networks using a GSM8K-style modular arithmetic on computational graphs task as a testbed.

### ğŸ”§ Key Contributions

We introduce and explore **four architectural mechanisms** aimed at enhancing OOD generalization:

1. **ğŸ”„ Input-adaptive recurrence** - Recurrent architecture that scales computation through input-adaptive recurrence.
2. **ğŸ“š Algorithmic supervision** - Structured learning objectives that encode algorithmic knowledge  
3. **âš“ Anchored latent representations** - Discrete bottlenecks for stable feature learning
4. **ğŸ”§ Explicit error-correction mechanism** - Built-in self-correction capabilities

### ğŸ¯ Results

Collectively, these mechanisms yield an architectural approach for **native and scalable latent space reasoning** in Transformer networks with robust algorithmic generalization capabilities. We complement these empirical results with a detailed **mechanistic interpretability analysis** that reveals how these mechanisms give rise to robust OOD generalization abilities.

---
<!-- 
## ğŸ—‚ï¸ Repository Structure

```
ğŸ“ algorithmic-generalization-transformer-architectures/
â”œâ”€â”€ ğŸ“„ LICENSE                          # Project license
â”œâ”€â”€ ğŸ“„ README.md                        # This file
â”œâ”€â”€ ğŸ“ experiments/                     # Main experimental code
â”‚   â”œâ”€â”€ ğŸ dag_generator.py            # Computational graph generation
â”‚   â”œâ”€â”€ ğŸ generate_data.py            # Data generation utilities
â”‚   â”œâ”€â”€ ğŸ model.py                    # Core model implementations
â”‚   â”œâ”€â”€ ğŸ“„ readme.md                   # Detailed experiment instructions
â”‚   â”œâ”€â”€ ğŸ tokenizers.py               # Custom tokenization methods
â”‚   â”œâ”€â”€ ğŸ train.py                    # Main training script
â”‚   â”œâ”€â”€ ğŸ utils.py                    # Utility functions
â”‚   â”œâ”€â”€ ğŸ“ baselines/                  # Baseline model implementations
â”‚   â”‚   â”œâ”€â”€ ğŸ baseline_cot_train.py   # Chain-of-thought baseline training
â”‚   â”‚   â”œâ”€â”€ ğŸ baseline_models.py      # Standard baseline architectures
â”‚   â”‚   â”œâ”€â”€ ğŸ baseline_train.py       # General baseline training
â”‚   â”‚   â””â”€â”€ ğŸ generate_cot_data.py    # CoT data generation
â”‚   â”œâ”€â”€ ğŸ“ checkpoints/                # Saved model checkpoints
â”‚   â”‚   â””â”€â”€ ğŸ“ demo_group-demo_run/    # Example checkpoint
â”‚   â”œâ”€â”€ ğŸ“ configs/                    # Runtime configuration files
â”‚   â”œâ”€â”€ ğŸ“ data/                       # Generated datasets
â”‚   â”‚   â””â”€â”€ ğŸ“ Tr32Test128-ADD/        # Example dataset (32â†’128 nodes, ADD operations)
â”‚   â”‚       â”œâ”€â”€ ğŸ—ƒï¸ train_data.pt       # Training dataset
â”‚   â”‚       â”œâ”€â”€ ğŸ—ƒï¸ val_data.pt         # Validation dataset
â”‚   â”‚       â””â”€â”€ ğŸ”§ tokenizer.pickle    # Associated tokenizer
â”‚   â”œâ”€â”€ ğŸ“ evaluation/                 # Model evaluation tools
â”‚   â”‚   â”œâ”€â”€ ğŸ eval_baseline_model.py  # Baseline model evaluation
â”‚   â”‚   â”œâ”€â”€ ğŸ eval_cot_baseline_model.py # CoT baseline evaluation
â”‚   â”‚   â”œâ”€â”€ ğŸ eval_model.py           # Main model evaluation
â”‚   â”‚   â”œâ”€â”€ ğŸ eval_nointerm_model.py  # No-intermediate evaluation
â”‚   â”‚   â”œâ”€â”€ ğŸ““ generate_cot_depth_generalization_datasets.ipynb # CoT dataset notebook
â”‚   â”‚   â”œâ”€â”€ ğŸ generate_cot_depth_generalization_datasets.py # CoT dataset script
â”‚   â”‚   â”œâ”€â”€ ğŸ generate_depth_generalization_datasets.py # Main dataset generation
â”‚   â”‚   â”œâ”€â”€ ğŸ metric_utils.py         # Evaluation metrics
â”‚   â”‚   â”œâ”€â”€ ğŸ“ results/                # Evaluation results
â”‚   â”‚   â””â”€â”€ ğŸ“ val_datasets/           # Generated evaluation datasets
â”‚   â”‚       â””â”€â”€ ğŸ“ Tr32Test128/        # Example evaluation data
â”‚   â”œâ”€â”€ ğŸ“ example_configs/            # Example configuration files
â”‚   â”‚   â”œâ”€â”€ ğŸ“ CoT/                    # Chain-of-Thought configurations
â”‚   â”‚   â”‚   â”œâ”€â”€ âš™ï¸ data_config.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ âš™ï¸ model_config.yaml
â”‚   â”‚   â”‚   â””â”€â”€ âš™ï¸ train_config.yaml
â”‚   â”‚   â”œâ”€â”€ ğŸ“ feedforward_or_recurrent/ # Baseline configurations
â”‚   â”‚   â”‚   â”œâ”€â”€ âš™ï¸ data_config.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ âš™ï¸ model_config.yaml
â”‚   â”‚   â”‚   â””â”€â”€ âš™ï¸ train_config.yaml
â”‚   â”‚   â””â”€â”€ ğŸ“ our_method/             # Our method configurations
â”‚   â”‚       â”œâ”€â”€ âš™ï¸ data_config.yaml
â”‚   â”‚       â”œâ”€â”€ âš™ï¸ model_config.yaml
â”‚   â”‚       â””â”€â”€ âš™ï¸ train_config.yaml
â”‚   â”œâ”€â”€ ğŸ“ lightning_logs/             # Training logs
â”‚   â””â”€â”€ ğŸ“ model_interp/               # Model interpretability analysis
â”‚       â”œâ”€â”€ ğŸ conduct_controlled_exp.py # Controlled experiments
â”‚       â”œâ”€â”€ ğŸ““ transformer_model_interpretation_documented.ipynb # Analysis notebook
â”‚       â”œâ”€â”€ ğŸ“ base_config-Nodes32-ADD/ # Interpretation configuration
â”‚       â”‚   â”œâ”€â”€ âš™ï¸ data_config.yaml
â”‚       â”‚   â”œâ”€â”€ âš™ï¸ model_config.yaml
â”‚       â”‚   â””â”€â”€ âš™ï¸ train_config.yaml
â”‚       â”œâ”€â”€ ğŸ“ demo_checkpoint/         # Pre-trained model checkpoint
â”‚       â”‚   â””â”€â”€ ğŸ’¾ last.ckpt
â”‚       â””â”€â”€ ğŸ“ figures/                 # Visualization outputs
â”‚           â”œâ”€â”€ ğŸŒ L0_head_view.html
â”‚           â””â”€â”€ ğŸŒ L0_model_view.html
â””â”€â”€ ğŸ“ Simtransformer/                 # Simulation framework
    â”œâ”€â”€ ğŸ“„ README.md                   # Framework documentation
    â”œâ”€â”€ ğŸ“ configurations/             # Global configuration templates
    â”‚   â”œâ”€â”€ âš™ï¸ data_config_default.yaml
    â”‚   â”œâ”€â”€ âš™ï¸ model_config_default.yaml
    â”‚   â”œâ”€â”€ âš™ï¸ probe_config_default.yaml
    â”‚   â””â”€â”€ âš™ï¸ train_config_default.yaml
    â””â”€â”€ ğŸ“ simtransformer/             # Core framework code
        â”œâ”€â”€ ğŸ manager.py              # Experiment management
        â”œâ”€â”€ ğŸ model_bank.py           # Model registry
        â”œâ”€â”€ ğŸ model_base.py           # Base model classes
        â”œâ”€â”€ ğŸ module_base.py          # Base module classes
        â”œâ”€â”€ ğŸ“„ README.md               # Framework details
        â”œâ”€â”€ ğŸ utils.py                # Framework utilities
        â””â”€â”€ ğŸ“ configurations/         # Local configurations
            â”œâ”€â”€ âš™ï¸ data_config_default.yaml
            â”œâ”€â”€ âš™ï¸ model_config_default.yaml
            â”œâ”€â”€ âš™ï¸ probe_config_default.yaml
            â””â”€â”€ âš™ï¸ train_config_default.yaml
``` -->

### ğŸ“‚ Organization of Codebase

- **`experiments/`** - Contains the main experimental code for training and evaluating models with the proposed architectural mechanisms
- **`experiments/baselines/`** - Baseline implementations for comparison including chain-of-thought models and standard transformers
- **`experiments/evaluation/`** - Code for evaluating the algorithmic generalization capabilities of different methods.
- **`experiments/model_interp/`** - Mechanistic interpretability analysis tools and visualizations
- **`Simtransformer/`** - Helper framework implementing Transformer modules and related utilities

See [`experiments/readme.md`](experiments/readme.md) for instructions on how to reproduce the experiments in the paper.

<!-- TODO -->
<!-- ## Citation

```bibtex
@article{altabaa2025unlocking,
  title = {Unlocking Out-of-Distribution Generalization in Transformers via Recursive Latent Space Reasoning},
  author = {Altabaa, Awni and Chen, Siyu and Yang, Zhuoran and Lafferty, John},
  year = {2025},
  journal = {arXiv preprint arxiv:[...]}
}
``` -->