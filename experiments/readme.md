# Instructions for Reproducing Experiments

## Our Method: Input-Adaptive Recurrence, Algorithmic Supervision, Anchored Discrete Latent Representations, & Error Corrections

### Data Generation

Generate the training and validation datasets using the `generate_data.py` script. This script takes a `yaml` config file as input that specifies different parameters of the data generation process and the underlying problem.

**Data Generation Overview.** Each problem instance in the task consists of a randomly generated computational graph with a certain number of nodes. Each non-leaf node is computed from previous nodes (with respect to topologicial order of DAG) via some arithmetic operations (Add, Subtract, or Multiply). Each sample is generated by randomly sampling the graph structure, randomly sampling leaf values, and randomly sampling operations. The inputs and outputs are represented as sequences of tokens. The model's objective is to compute the values of *all* nodes in the graph. We train the model on graphs with some maximum size $N_{tr}$ and evaluate them on larger graphs $N_{test} > N_{tr}$.

**Example Data Configuration File:**
```yaml
# example_configs/our_method/data_config.yaml
dag_config:
  data_file_name: train_data.pt
  func_vocab: ['ADD']
  num_nodes: 32
  num_samples: 50000
  max_length: 256
  mod_val: 23
  num_leaf_nodes: 6
  max_depth: 6
  max_comp_depth: 6
  max_fan_in_deg: 3
  min_fan_in_deg: 1
  shuffle_node_name: true
  fix_graph: false
  verbose: false

val_dag_config:
  data_file_name: val_data.pt
  func_vocab: ['ADD']
  num_nodes: 128
  num_samples: 1000
  max_length: 1024
  mod_val: 23
  num_leaf_nodes: 6
  max_depth: 36
  max_comp_depth: 36
  max_fan_in_deg: 3
  min_fan_in_deg: 1
  shuffle_node_name: true
  fix_graph: false
  verbose: false

data_dir: data/Tr32Test128-ADD
tokenizer_file_name: tokenizer.pickle
```

**Configuration Parameters Explained:**
- `num_nodes`: Number of variables in the computational graph (controls problem size and complexity)
- `func_vocab`: List of operations to include in the computational graphs
- `max_length`: Maximum sequence length for tokenized expressions (for padding)
- `num_samples`: Number of computational graph samples to generate
- `mod_val`: Modular arithmetic base for numerical computations
- `num_leaf_nodes`: Number of input variables (leaves) in each graph
- `max_depth`/`max_comp_depth`: Maximum depth of the computational graph structure
- `max_fan_in_deg`/`min_fan_in_deg`: Range for number of inputs each operation can have
- `shuffle_node_name`: Whether to randomize variable names in expressions
- `fix_graph`: Whether to use a fixed graph structure across samples (False in experiments)
- `data_dir`: Output directory for generated datasets
- `tokenizer_file_name`: Name for the saved tokenizer file associated with this config

**Running Data Generation:**

```bash
cd experiments
python generate_data.py --data_config_path example_configs/our_method/data_config.yaml
```

This will generate the training (and validation) data and save it to `data_dir`, together with the tokenizer associated with this task.

### Training

Different configurations of our proposed method are implemented in `model.py`. Train the model using the `train.py` script with the configuration files (`data_config`, `model_config`, and `train_config`).

**Running Training:**

```bash
cd experiments
python train.py --config_dir example_configs/our_method
```

This command will:
- Load the model, training, and data configurations from the specified directory
- Initialize the model with according to the model configuration
- Train the model with algorithmic supervision and error correction (according to config)
- Save checkpoints to `experiments/checkpoints/` directory
- Log training metrics to Weights & Biases (if configured)

**Configurable Architectural Mechanisms:**
Our proposed method includes four key architectural mechanisms:
1) **Input-Adaptive Recurrence**: Model dynamically adjusts computation depth based on problem complexity
2) **Algorithmic Supervision**: Supervision signal at each iteration to align with algorithmic solution
3) **Discretized Intermediate Representations**: Intermediate representations pass througha discrete latent bottleneck at each iteration. (Turned off by `nointerm: true` in `model_config`)
4) **Error Correction**: Model is trained to detect and correct errors through randomly injected errors in intermediate states (configured by `recorrection_args` in `train_config`)

**Training Configuration Options:**

```bash
# Train model
python DAG_train.py --config_dir example_configs/our_method
```

**Example Model & Training Configuration Files:**

*Model Configuration (`model_config.yaml`):*
```yaml
# Model architecture parameters
nointerm: false               # Turn off discrete latent states by setting this to true
hidden_size: 384              # Transformer hidden dimension
num_layers: 4                 # Number of transformer layers in recurrent block
num_heads: 16                 # Number of attention heads
max_seq_len: 1024             # Maximum sequence length
pos_enc_type: DeBERTa         # Positional encoding type (DeBERTa relative positions)
attention_type: softmax       # Attention mechanism
causal_attn: true             # Use causal attention
attn_pdrop: 0.1               # Attention dropout rate
resid_pdrop: 0.1              # Residual connection dropout rate
use_layer_norm: true          # Enable layer normalization
use_readout_proj: true        # Use projection layer for output
```

*Training Configuration (`train_config.yaml`):*
```yaml
# Training hyperparameters
max_epochs: 100              # Maximum number of training epochs
batch_size: 32               # Training batch size
learning_rate: 0.001         # Base learning rate
optimizer: AdamW             # Optimizer (AdamW, Adam, SGD, etc.)
lr_scheduler: none           # Learning rate scheduler
precision: 32                # Training precision (16, 32, bf16)
compile: true                # Enable model compilation for speed

# Our method specific parameters
max_train_loop: 8            # Maximum recurrent loops during training
max_val_loop: 36             # Maximum recurrent loops during validation
use_teacher_pred_for_next_loop: true  # Use teacher forcing for stability
use_fast_batch_forward: true # Enable fast batch processing

# Error correction parameters
recorrection_args:
  enable: true               # Enable error correction mechanism
  corruption_prob: 0.05      # Probability of introducing errors
  corruption_strategy: last  # Where to introduce errors ('last', 'random')
  tokenwise_corruption_prob: 0.5  # Token-level corruption probability

# Monitoring and logging
wandb_config:
  wandb_entity: [YOUR_ENTITY]
  wandb_project: [YOUR_PROJECT]
```

### Evaluation

Here, we evaluate the OoD algorithmic generalization capabilities of our models. We first randomly generate validation datasets at different graph sizes, starting at sizes smaller than the training regime and going up to sizes much larger than the training regime. We then evaluate the models' performance on each problem size to assess its algorithmic generalization capabilities.

#### Generating Out-of-Distribution Evaluation Datasets

To test algorithmic generalization, we need evaluation datasets with varying computational graph sizes. These can be generated using `generate_depth_generalization_datasets.py`:

```bash
cd evaluation
python generate_depth_generalization_datasets.py \
    --data_config_path ../example_configs/our_method/data_config.yaml \
    --min_nodes 8 \
    --increment 8 \
    --num_samples 1024
```

**Script Parameters:**
- `--data_config_path`: Path to the data configuration file (uses same config as training)
- `--min_nodes`: Minimum number of nodes for evaluation datasets (default: 8)
- `--increment`: Step size for increasing node counts (default: 8)  
- `--num_samples`: Number of samples per evaluation dataset (default: 1024)

This generates evaluation datasets for node counts from `min_nodes` up to the maximum of number of nodes specified by the config, creating datasets with computational graphs of varying complexity. The output directory structure will be:

```
evaluation/val_datasets/Tr{train_nodes}Test{val_nodes}/{func_vocab}/
├── testdata_N8.pt     # 8-node evaluation dataset
├── testdata_N16.pt    # 16-node evaluation dataset  
├── testdata_N24.pt    # 24-node evaluation dataset
└── ...                # Additional datasets up to max nodes
```

Each dataset file contains:
- `tokenized_dataset`: Tokenized input sequences for the computational graphs
- `dataset_info`: Metadata including computation depths and node information
- `spec`: Dataset statistics (min/mean/max depth, sequence lengths)

#### Evaluating Algorithmic Generalization

Once you have trained a model and generated evaluation datasets, evaluate the model's generalization capabilities:

```bash
cd evaluation
python eval_model.py \
    --group_name "Nodes32-ADD - MethodName" \
    --run_name "seed-123456789 - 2024-01-01-12:00:00" \
    --val_ds_path val_datasets/Tr32Test128 \
    --out_path results/our_method \
    --batch_size 16 \
    --ckpt_name last.ckpt
```

**Key Parameters:**
- `--group_name`: Wandb group name from training (identifies model configuration)
- `--run_name`: Wandb run name from training (identifies specific training run; )
- `--val_ds_path`: Path to validation datasets directory
- `--out_path`: Output directory for evaluation results
- `--batch_size`: Batch size for evaluation (default: 1)
- `--ckpt_name`: Checkpoint filename to load (default: last.ckpt)

**Evaluation Outputs:**

The evaluation script generates comprehensive results in `{out_path}/{group_name}/{run_name}/`.


## Baselines

The procedure is very similar for the baselines, but sometimes uses different scripts to implement the different steps. It is briefly outlined below.

### Feedforward & Recurrent Models

These baselines use standard feedforward or recurrent transformer architectures without the specialized mechanisms of our method (no input-adaptive recurrence, algorithmic supervision, discrete intermediate representations, or error correction).

#### Data Generation

**Key Difference:** Uses the same data generation process as our method but with simpler configurations.

```bash
cd experiments
python generate_data.py --data_config_path example_configs/feedforward_or_recurrent/data_config.yaml
```

The configuration is nearly identical to our method, but without specialized training features. The data generation produces the same computational graph datasets for consistent comparison.

#### Training

Training is performed using `baseline_train.py`.

```bash
cd experiments
python baselines/baseline_train.py --config_dir example_configs/feedforward_or_recurrent
```

**Model Configuration Differences:**
The baseline models use standard transformer architectures:
- No input-adaptive recurrence (fixed computation depth)
- No intermediate supervision or discrete latent states
- No error correction mechanisms

#### Evaluation

Evaluation is performed using the `eval_baseline_model.py` script:

```bash
cd evaluation
python eval_baseline_model.py \
    --group_name "Baseline_Group_Name" \
    --run_name "baseline_run_name" \
    --val_ds_path val_datasets/Tr32Test128 \
    --out_path results/baselines \
    --batch_size 16 \
    --ckpt_name last.ckpt
```

Uses the same evaluation datasets generated by `generate_depth_generalization_datasets.py` for fair comparison with our method.

### Chain-of-Thought

Chain-of-Thought baselines use a form "intermediate supervision" by providing step-by-step computational traces during training (in token space), but without the adaptive recurrence, discretized intermediate representations, or error correction of our method.

#### Data Generation

For the chain-of-thought baselines, we randomly sample computation graphs as in `generate_data.py` above, then generate CoT traces to train on. This is done using `generate_cot_data.py`.

```bash
cd experiments
python baselines/generate_cot_data.py --data_config_path example_configs/CoT/data_config.yaml
```

**CoT-Specific Configuration Parameters:**
- `cot: true` - Enables Chain-of-Thought data generation
- `cot_type: eq-val` - Type of intermediate supervision (equation-value pairs)

For evaluation dataset generation:
```bash
cd evaluation
python generate_cot_depth_generalization_datasets.py \
    --data_config_path ../example_configs/CoT/data_config.yaml \
    --min_nodes 8 \
    --increment 8 \
    --num_samples 1024 \
    --cot_type eq-val
```

#### Training

Training of the CoT baselines is performed using the `baseline_cot_train.py` script.

```bash
cd experiments
python baselines/baseline_cot_train.py --config_dir example_configs/CoT
```

#### Evaluation

The CoT baseline models are evaluated using the `eval_cot_baseline_model.py` script.

```bash
cd evaluation
python eval_cot_baseline_model.py \
    --group_name "CoT_Group_Name" \
    --run_name "cot_run_name" \
    --val_ds_path val_datasets/CoT \
    --out_path results/cot_baselines \
    --batch_size 16 \
    --ckpt_name last.ckpt
```

**CoT Evaluation Process:**
- Uses CoT evaluation datasets generated by `generate_cot_depth_generalization_datasets.py`
- Evaluates step-by-step generation accuracy
- Measures performance on intermediate computational steps
- Compares against ground-truth computational traces